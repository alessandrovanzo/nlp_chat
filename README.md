# NLP Chat - RAG-based Knowledge Base Chatbot

A sophisticated RAG (Retrieval-Augmented Generation) chatbot with document upload capabilities. Upload PDF, EPUB, and TXT documents to create a searchable knowledge base powered by OpenAI embeddings.

## ğŸ¯ Features

- **Multi-format Support**: Upload PDF, EPUB, and TXT documents
- **Semantic Search**: Uses OpenAI embeddings for intelligent document retrieval
- **Chat Interface**: Beautiful Chainlit-powered chat UI
- **Web Upload**: Browser-based document upload and management
- **Source Management**: Toggle documents on/off, view metadata, delete sources
- **MCP Protocol**: FastAPI backend implements Model Context Protocol for tool calling

## ğŸ“ Project Structure

```
nlp_chat/
â”œâ”€â”€ src/                          # Main application code
â”‚   â”œâ”€â”€ config.py                 # Configuration (loads from .env)
â”‚   â”œâ”€â”€ chat/                     # Chainlit chat interface
â”‚   â”‚   â””â”€â”€ app.py
â”‚   â”œâ”€â”€ server/                   # FastAPI backend
â”‚   â”‚   â”œâ”€â”€ api.py
â”‚   â”‚   â””â”€â”€ static/
â”‚   â”‚       â””â”€â”€ upload.html       # Document upload interface
â”‚   â”œâ”€â”€ document/                 # Document processing
â”‚   â”‚   â”œâ”€â”€ extractors.py         # Extract text from files
â”‚   â”‚   â”œâ”€â”€ chunker.py            # Chunk text for embedding
â”‚   â”‚   â””â”€â”€ embeddings.py         # Generate OpenAI embeddings
â”‚   â””â”€â”€ database/                 # Database operations
â”‚       â”œâ”€â”€ models.py             # DB operations and document processing
â”‚       â””â”€â”€ init_db.py            # Database initialization
â”œâ”€â”€ scripts/                      # Utility scripts
â”‚   â”œâ”€â”€ start_server.py           # Start FastAPI server
â”‚   â”œâ”€â”€ start_chat.py             # Start Chainlit chat
â”‚   â””â”€â”€ inspect_chunk.py          # Debug utility
â”œâ”€â”€ data/                         # Data directory
â”‚   â””â”€â”€ rag_database.db           # SQLite database
â”œâ”€â”€ .env                          # Environment variables (create from .env.example)
â”œâ”€â”€ .env.example                  # Template for environment variables
â”œâ”€â”€ requirements.txt              # Python dependencies
â””â”€â”€ README.md                     # This file
```

## ğŸš€ Setup

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

### 2. Configure Environment

Create a `.env` file from the template:

```bash
cp .env.example .env
```

Edit `.env` and add your OpenAI API key:

```
OPENAI_API_KEY=your-openai-api-key-here
MCP_SERVER_URL=http://localhost:8001
```

### 3. Initialize Database (Optional)

The database will be created automatically when you first start the server, but you can initialize it manually:

```bash
python -m src.database.init_db
```

## ğŸ® Usage

### Start the Backend Server

```bash
python scripts/start_server.py
```

The server will start on `http://localhost:8001`

**Upload Interface**: `http://localhost:8001/upload`

### Start the Chat Interface

In a separate terminal:

```bash
python scripts/start_chat.py
```

The chat interface will start on `http://localhost:8000`

### Upload Documents

1. Navigate to `http://localhost:8001/upload`
2. Upload a PDF, EPUB, or TXT file
3. Provide a source name and description
4. Adjust chunking settings (pages per chunk)
5. Submit and wait for processing

### Chat with Your Knowledge Base

1. Navigate to `http://localhost:8000`
2. Ask questions related to your uploaded documents
3. The AI will automatically search the knowledge base and provide answers with sources

## ğŸ”§ Configuration

### Chunking Settings

- **Pages per Chunk**: Controls how many pages are grouped together (1-10)
  - Smaller chunks: More precise retrieval, cheaper embeddings, less context
  - Larger chunks: More context, more expensive, potentially less precise

- **Prepend Metadata**: Includes document title and description in embeddings
  - Helps with retrieval when searching by document topic
  - Slightly increases embedding costs

### Document Processing

- **PDF**: Uses actual page numbers
- **EPUB**: Converts to virtual pages (300 words = 1 page)
- **TXT**: Converts to virtual pages (300 words = 1 page)

## ğŸ“Š Database Schema

### `documents` Table
- Stores document metadata
- Tracks active/inactive status
- Maintains chunk count

### `chunks` Table
- Stores document chunks with embeddings
- Links to parent document (CASCADE DELETE)
- Includes page ranges and metadata

## ğŸ› ï¸ Utility Scripts

### Inspect a Chunk

```bash
python scripts/inspect_chunk.py <chunk_id>
```

Displays detailed information about a specific chunk, including content and metadata.

## ğŸ” Security

- API keys stored in `.env` (excluded from git)
- Never commit `.env` file
- Database excluded from git

## ğŸ“ Technology Stack

- **Frontend**: Chainlit (chat UI)
- **Backend**: FastAPI
- **Database**: SQLite with foreign keys
- **Embeddings**: OpenAI text-embedding-3-small
- **LLM**: GPT-4 Turbo (configurable)
- **Document Processing**: PyPDF2, ebooklib, BeautifulSoup4

## ğŸ¤ Architecture

```
User Query â†’ Chainlit Chat Interface
              â†“
          OpenAI GPT-4 (with function calling)
              â†“
       [Detects need for knowledge]
              â†“
          FastAPI MCP Server
              â†“
       Generate Query Embedding (OpenAI)
              â†“
       Search Database (Cosine Similarity)
              â†“
       Return Top K Results with Sources
              â†“
          Back to GPT-4 for Answer
              â†“
       Display Answer with Sources
```

## ğŸ“š API Endpoints

### FastAPI Server (Port 8001)

- `GET /` - Health check
- `GET /upload` - Upload interface (HTML)
- `GET /mcp/tools` - List available MCP tools
- `POST /mcp/tools/call` - Execute MCP tool
- `POST /upload-pdf` - Upload and process document
- `GET /sources` - List all documents
- `POST /sources/toggle` - Toggle document active status
- `POST /sources/delete` - Delete document and chunks

## ğŸ› Troubleshooting

### Database Issues

If you encounter database errors, try reinitializing:

```bash
rm data/rag_database.db
python -m src.database.init_db
```

### Import Errors

Make sure you're running scripts from the project root directory.

### Token Limit Errors

If documents are too large, the system will automatically split chunks. You can also:
- Reduce "pages per chunk" setting
- Turn off "prepend metadata"

## ğŸ“„ License

This project is provided as-is for educational and personal use.

## ğŸ™ Acknowledgments

Built with:
- [Chainlit](https://github.com/Chainlit/chainlit) - Chat UI framework
- [FastAPI](https://fastapi.tiangolo.com/) - Modern web framework
- [OpenAI](https://openai.com/) - Embeddings and LLM

